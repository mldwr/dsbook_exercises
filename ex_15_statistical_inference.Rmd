---
title: "Ex 15 Statistical Inference"
output:
  html_document:
    df_print: paged
    number_sections: true
    toc: true
    pandoc_args: [
      "--number-sections",
      "--number-offset=15"
      ]
---

## Exercises: Parameters Estimates

### Qn

>1\. Suppose you poll a population in which a proportion $p$ of voters are Democrats and $1-p$ are Republicans. Your sample size is $N=25$. Consider the random variable $S$ which is the **total** number of Democrats in your sample. What is the expected value of this random variable? Hint: it's a function of $p$.

The definition of the average is defined as the sum of its elements devided by the number of its elements. 

$$
\bar{X} = \frac{1}{N} \times \sum_{i=1}^N x_i = \frac{1}{N} X_N \Leftrightarrow N \bar{X} = X_N
$$
The expectation value is defined in the book as $\mbox{E}(\bar{X}) = p$. inserting the value $X=N\bar{X}$ into the standard Error one yields the following relation. 

$$
\mbox{E}(X) = N\ \mbox{E}(\bar{X}) = Np
$$
Similarly, for $S=N\bar{S}$ and $N=25$ one can write the following expression, where expected value is $25p$ denoting the number of Democrats for the sample size of 25 people. 
$$
\mbox{E}(S) = N\ \mbox{E}(\bar{S}) = Np = 25p
$$

### Qn
>2\. What is the standard error of $S$ ? Hint: it's a function of $p$.

Using the definition of the standard error of $\bar{X}$ as $\mbox{SE}(\bar{X}) = \sqrt{p(1-p)/N}$ and the definition $S=N\ \bar{S}$ one can write the following equation.

$$
\mbox{SE}(S) = N\ \mbox{SE}(\bar{S}) = N \sqrt{p(1-p)/N} = \sqrt{Np(1-p)} = \sqrt{25p(1-p)}
$$

### Qn
>3\. Consider the random variable $S/N$. This is equivalent to the sample average, which we have been denoting as $\bar{X}$. What is the expected value of the $\bar{X}$? Hint: it's a function of $p$.

Defining the random variable as $\bar{X} = S/N$ and utilizing the definition $\mbox{E}(S) = Np$ one can write the following equation.

$$
\mbox{E}(\bar{X}) = \mbox{E}(S/N) = \mbox{E}(S)\ \frac{1}{N} =  Np\ \frac{1}{N}= p
$$

### Qn
>4\. What is the standard error of $\bar{X}$? Hint: it's a function of $p$.

The standard error as a function of $\bar{X}$ is also provided in the book in chapter 15.2.4 "*Properties of our estimate: expected value and standard error*".
$$
\mbox{SE}(\bar{X}) = \sqrt{p(1-p)/N}
$$

### Qn
>5\. Write a line of code that gives you the standard error `se` for the problem above for several values of $p$, specifically for `p <- seq(0, 1, length = 100)`. Make a plot of `se` versus `p`. 

From this plot one can see the function $\mbox{SE[X(p)]}$ evolving by varying the parameter $p$. The function we see is symmetric. For probabilities p=0 and p=1 the standard error is zero, therefore for events with absolute certainty of occurring =1 or not occurring =0 there is no room for deviations. For the probability of p=0.5 the standard error is at its maximum, meaning that this point marks the biggest uncertainty for the probability. The standard error and therefore the uncertainty is falling off as p moves towards one of the certain points. 

```{r,message=FALSE}
library(tidyverse)
N <- 25
p<-seq(0,1,length=100)
se<-sqrt(p*(1-p)/N)
qplot(p,se)
````


### Qn
>6\. Copy the code above and put it inside a for-loop to make the plot for $N=25$, $N=100$, and $N=1000$. 

In the following plots it can be noticed that the standard error "se" gets smaller for increasing population size of N (from red to blue in the bottom plot).

```{r,message=FALSE}
vN<-c(25,100,1000)
p<-seq(0,1,length=100)
for(N in vN){
se<-sqrt(p*(1-p)/N)
plot(p,se,ylim = c(0,0.5/sqrt(25)))
}
````
```{r,message=FALSE}
library(tidyverse)
N25<-replicate(100,25)
N1000<-replicate(100,1000)
N100<-replicate(100,100)
N<-c(N25,N100,N1000)
p<-seq(0,1,length=100)
se<-sqrt(p*(1-p)/N)
df <- data.frame(p,se,N) %>% mutate(N=as.factor(N))
df %>% ggplot(aes(p,se,color=N)) + geom_point()
````

### Qn
>7\. If we are interested in the difference in proportions, $p - (1-p)$, our estimate is $d = \bar{X} - (1-\bar{X})$. Use the rules we learned about sums of random variables and scaled random variables to derive the expected value of $d$.

Here, the lower case letter $d$ denotes the spread and is defined as $d = \bar{X} - (1-\bar{X}) = 2\bar{X}-1$. \ The following auxiliary equation from previous exercise is used $E[\bar{X}]=p$. \
In addition the following holds true that the expectation value of a constant is the constant itself $E[a]=a$. \
Finally the expectation value obeys linear transformation such as $E[aX]=aE[X]$.

$$
\mbox{E}[d] = \mbox{E}[\bar{X} - (1-\bar{X})] =  \mbox{E}[\bar{X} - (1-\bar{X})] = \mbox{E}[2\bar{X} - 1] = \mbox{E}[2\bar{X}] - E[1] = 2 \mbox{E}[\bar{X}] - E[1] = 2p-1
$$

### Qn
>8\. What is the standard error of $d$?

First, a few auxiliary formulas should be recalled. \
Again, the spread is defined as $d = 2\bar{X}-1$. \
Then, the standard error does obey linear transformation, which means that $\mbox{SE}[aX]=a\mbox{SE}[X]$. \
Also, the standard error of a constant is zero $\mbox{SE}[a]=0$. \
Recall from a previous exercise that $\mbox{SE}(\bar{X}) = \sqrt{p(1-p)/N}$ holds true. 
 

$$
\mbox{SE}[d] = \mbox{SE}[\bar{X} - (1-\bar{X})] = \mbox{SE}[2\bar{X} - 1] = \mbox{SE}[2\bar{X}] - \mbox{SE}[1] = 2\mbox{SE}[\bar{X}] - \mbox{SE}[1] =2\sqrt{p(1-p)/N}
$$


### Qn
>9\. If the actual $p=.45$, it means the Republicans are winning by a relatively large margin since $d= -.1$, which is a 10% margin of victory. In this case, what is the standard error of $2\hat{X}-1$ if we take a sample of $N=25$? 

The expectation value of the spread $d$ is 10% (E[d]=0.1), where the standard error is about 20% (SE[d]=0.199). Therefore, the estimated error is double in size than the actual measurement. 

```{r,message=FALSE}
N<-25
p<-0.45
Ed<-2*p-1
SEd<-2*sqrt(p*(1-p)/N)
Ed
SEd
````


### Qn
>10\. Given the answer to 9, which of the following best describes your strategy of using a sample size of $N=25$?
>
>a. The expected value of our estimate $2\bar{X}-1$ is $d$, so our prediction will be right on.
>b. Our standard error is larger than the difference, so the chances of $2\bar{X}-1$ being positive and throwing us off were not that small. We should pick a larger sample size.
>c. The difference is 10% and the standard error is about 0.2, therefore much smaller than the difference.
>d. Because we don't know $p$, we have no way of knowing that making $N$ larger would actually improve our standard error.

Answer b, our standard error is larger than the difference, so the chances of $2\bar{X}-1$ being positive and throwing us off were not that small. We should pick a larger sample size. \
This is the direct consequence of the answer from the last question. The standard error is inversely proportional $\mbox{SE} \sim 1/N$ to the sample size, as stated in the answer of question 8. Therefore, the larger the sample size the smaller the standard error will be. 

## Exercises: CLT

### Qn

>1\. Write an _urn model_ function that takes the proportion of Democrats $p$ and the sample size $N$ as arguments and returns the sample average if Democrats are 1s and Republicans are 0s. Call the function `take_sample`. 

The used function is `sample()`, which is already utilized multiple times in past questions.
For demonstration purposes the parameters used here are $N=100$ for the population size and $p=0.45$ for the probability, as suggested in the next question. In order to gain the same value for all runs the function `set.seed(1)` is implemented. The expectation value results in 0.46.

```{r}
N<-100
p<-0.45

set.seed(1)

take_sample <- function(p,N) {
  x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  mean(x)
}
take_sample(p,N)
```

### Qn
>2\. Now assume `p <- 0.45` and that your sample size is $N=100$. Take a sample 10,000 times and save the vector of `mean(X) - p` into an object called `errors`. Hint: use the function you wrote for exercise 1 to write this in one line of code.

The question is asking to construct a Monte Carlo simulation. The function `replicate()` is used for this purpose. The plot resembles nicely a normal like distribution of the random variables. 

```{r}
N<-100
p<-0.45
B<-10000

set.seed(1)
errors <- replicate(B, {
  X <- take_sample(p,N)
  mean(X)-p
})
qplot(errors,bins=10)
```

### Qn
>3\. The vector `errors` contains, for each simulated sample, the difference between the actual $p$ and our estimate $\bar{X}$. We refer to this difference as the _error_. Compute the average and make a histogram of the errors generated in the Monte Carlo simulation and select which of the following best describes their distributions:
>
>```{r, eval=FALSE}
>mean(errors)
>hist(errors)
>```
>
>a. The errors are all about 0.05.
>b. The errors are all about -0.05.
>c. The errors are symmetrically distributed around 0.
>d. The errors range from -1 to 1.

Answer c, the errors are symmetrically distributed around 0. \
This can be seed by computing the average and plotting a histogram. 

```{r}
mean(errors)
hist(errors)
```

### Qn
>4\. The error $\bar{X}-p$ is a random variable. In practice, the error is not observed because we do not know $p$. Here we observe it because we constructed the simulation.  What is the average size of the error if we define the size by taking the absolute value $\mid \bar{X} - p \mid$ ?

The average size of the error $\mid \bar{X} - p \mid$ is 0.04. What happens here can be illustrated by a histogram plot. It shows that the values no longer accumulate around zero, but are moved to the positive values and are amplified by its previously negative counterparts. 

```{r}
N<-100
p<-0.45
B<-10000

set.seed(1)
errors <- replicate(B, {
  X <- take_sample(p,N)
  p-mean(X)
})
mean(abs(errors))
hist(abs(errors))
```


### Qn
>5\. The standard error is related to the typical **size** of the error we make when predicting. We say **size** because we just saw that the errors are centered around 0, so thus the average error value is 0. For mathematical reasons related to the Central Limit Theorem, we actually use the standard deviation of `errors` rather than the average of the absolute values to quantify the typical size. What is this standard deviation of the errors?

The standard deviation can be written as the square root of the expectation value of the error as follows: $\mbox{SD}[p-\bar{X}] = \sqrt{E[(p-\bar{X})^2]}$. The resulting standard deviation is about 0.05 in magnitude.

```{r}
N<-100
p<-0.45
B<-10000
set.seed(1)

errors <- replicate(B, {
  X <- take_sample(p,N)
  p-mean(X)
})
sqrt(mean(errors^2))
```


### Qn
>6\. The theory we just learned tells us what this standard deviation is going to be because it is the standard error of $\bar{X}$. What does theory tell us is the standard error of $\bar{X}$ for a sample size of 100?

The standard error is defined as figured in question 1.4: $\mbox{SE}(p) = \sqrt{p(1-p)/N}$. The value results in a magnitude of 0.05. The same value is yielded in previous question, when a Monte Carlo simulation was used. 

```{r}
N<-100
p<-0.45
set.seed(1)

se <- sqrt(p*(1-p)/N)
se
```


### Qn
>7\. In practice, we don't know $p$, so we construct an estimate of the theoretical prediction based by plugging in $\bar{X}$ for $p$. Compute this estimate. Set the seed at 1 with `set.seed(1)`.

Swapping the p-value with the average $\bar{X}$ in the equation from the standard error the following expression is gained $\mbox{SE}(\bar{X}) = \sqrt{\bar{X}(1-\bar{X})/N}$. Again, the same value is yielded as in the past two question in the magnitude of 0.05.

```{r}
N<-100
p<-0.45
set.seed(1)

X <- take_sample(p,N)
se <- sqrt(X*(1-X)/N)
se
```

### Qn
>8\. Note how close the standard error estimates obtained from the Monte Carlo simulation (exercise 5), the theoretical prediction (exercise 6), and the estimate of the theoretical prediction (exercise 7) are. The theory is working and it gives us a practical approach to knowing the typical error we will make if we predict $p$ with $\bar{X}$. Another advantage that the theoretical result provides is that it gives an idea of how large a sample size is required to obtain the precision we need. Earlier we learned that the largest standard errors occur for $p=0.5$. Create a plot of the largest standard error for $N$ ranging from 100 to 5,000. Based on this plot, how large does the sample size have to be to have a standard error of about 1%?
>
>a. 100
>b. 500
>c. 2,500
>d. 4,000

Answer c: 2,500 \
In the plot below there is a black horizontal line highlighting the 1% error border. The highest error at p=0.5 is only reached by the N values below a sample size of 2,500.

Results so far from past exercises for different calculations of the standard error. \
Q5 Monte Carlo simulation: 0.04949939 \
Q6 theoretical prediction: 0.04974937 \
Q7 estimate of theoretical prediction: 0.04983974 \

```{r,message=FALSE}
library(tidyverse)
N25<-replicate(100,25)
N100<-replicate(100,100)
N500<-replicate(100,500)
N1000<-replicate(100,1000)
N2500<-replicate(100,2500)
N4000<-replicate(100,4000)
N5000<-replicate(100,5000)

N<-c(N25,N100,N500,N1000,N2500,N4000,N5000)
p<-seq(0,1,length=100)
se<-sqrt(p*(1-p)/N)
df <- data.frame(p,se,N) %>% mutate(N=as.factor(N))
df %>% ggplot(aes(p,se,color=N)) + geom_point() + scale_y_continuous(breaks = seq(0, 0.1, by = 0.01)) + geom_segment(aes(x = 0, y = 0.01, xend = 1, yend = 0.01), color="black",size=0.3) + geom_segment(aes(x = 0.5, y = 0, xend = 0.5, yend = 0.1), color="black",size=0.3)
````

```{r,message=FALSE}
library(tidyverse)
N <- seq(100, 5000, len = 50)
p <- 0.5
se <- sqrt(p*(1-p)/N)
df <- data.frame(N,se) #%>% mutate(N=as.factor(N))
df
df %>% ggplot(aes(N,se)) + geom_point() + scale_y_continuous(breaks = seq(0, 0.1, by = 0.01)) + geom_segment(aes(x = 0, y = 0.01, xend = 5000, yend = 0.01), color="red",size=0.3) + geom_segment(aes(x = 2500, y = 0.005, xend = 2500, yend = 0.015), color="red",size=0.3)
```

### Qn
>9\. For sample size $N=100$, the central limit theorem tells us that the distribution of $\bar{X}$ is:
>
>a. practically equal to $p$.
>b. approximately normal with expected value $p$ and standard error $\sqrt{p(1-p)/N}$.
>c. approximately normal with expected value $\bar{X}$ and standard error $\sqrt{\bar{X}(1-\bar{X})/N}$.
>d. not a random variable.

Answer b, approximately normal with expected value $p$ and standard error $\sqrt{p(1-p)/N}$.\
This result is deduced in the book in chapter 15.4 "Central Limit Theorem in practice": *In summary, we have that $\bar{X}$ has an approximately normal distribution with expected value $p$ and standard error $\sqrt{p(1-p)/N}$.*

### Qn
>10\. Based on the answer from exercise 8, the error $\bar{X} - p$ is:
>
>a. practically equal to 0.
>b. approximately normal with expected value $0$ and standard error $\sqrt{p(1-p)/N}$.
>c. approximately normal with expected value $p$ and standard error $\sqrt{p(1-p)/N}$.
>d. not a random variable.

Answer b, approximately normal with expected value $0$ and standard error $\sqrt{p(1-p)/N}$.\
This is best seen in the answers of questions 2 and 3, where plots show a normal distribution around 0. This is because it was found in answer to question 1, that the mean of $\bar{X}$ is approximately p.

### Qn
>11\. To corroborate your answer to exercise 9, make a qq-plot of the `errors` you generated in exercise 2 to see if they follow a normal distribution. 

The qq-plot tells that the data from the `errors` vector indeed follows a normal distribution, because the plotted values follow a straight line (blue). 

```{r,message=FALSE}
library(tidyverse)
N<-100
p<-0.45
B<-10000

errors <- replicate(B, {
  X <- take_sample(p,N)
  p-mean(X)
})
df<-data.frame(sz=seq(1,B),errors)
ggplot(df,aes(sample=scale(errors))) + geom_qq() + geom_abline(color="blue")
```


### Qn
>12\. If $p=0.45$ and $N=100$ as in exercise 2, use the CLT to estimate the probability that $\bar{X}>0.5$. You can assume you know $p=0.45$ for this calculation.

In other words the question is asking: \
What is the probability that $\bar{X}>0.5$ in mathematical terms $Pr(\bar{X}>0.5)=p$? \
According to the definition (in ch. 13.10 "*Continuous probability*") of the probability distribution $Pr(\bar{X}>0.5)$ can be written as $Pr(\bar{X}>0.5)=1-Pr(\bar{X}\leq 0.5)$. 
Assuming the mean equals to $p$ ($m = p$), the standard deviation to the standard error ($\mbox{sd}=\mbox{se}$)  and the measured quantity to $\bar{X}$ ($x=\bar{X}$). One can utilize the `pnorm(x,m,sd)` function to compute the probability as $\mbox{pnorm}(\bar{X},p,\mbox{se})$. \
The calculation results in a probability of 16% that $\bar{X}$ is $\bar{X}>0.5$.


```{r}
N<-100
p<-0.45
xbar<-0.5

se <- sqrt(p*(1-p)/N)

1-pnorm(xbar,p,se)
```
```{r}
N<-100
p<-0.45
xbar<-0.5
vxbar<-seq(0,1,len=100)

se <- sqrt(p*(1-p)/N)

vpnorm<-1-pnorm(vxbar,p,se)
vdnorm<-dnorm(vxbar,p,se)

qplot(vxbar,vpnorm,geom=c("point","line"))

df<-data.frame(vxbar,vdnorm)

ggplot(df,aes(vxbar,vdnorm)) +
  stat_function(fun = dnorm, args = list(mean = p, sd = se), color="blue") +
  geom_point(aes(x=vxbar,y=vdnorm),shape=3, color="black", size=0.2) +
  geom_ribbon(data=subset(df,vxbar>=xbar),aes(ymin=0,ymax=vdnorm),fill="green", alpha=0.5)

```

### Qn
>13\. Assume you are in a practical situation and you don't know $p$. Take a sample of size $N=100$ and obtain a sample average of $\bar{X} = 0.51$. What is the CLT approximation for the probability that your error is equal to or larger than 0.01?


This topic is explained in the book chapter 15.4 "*Central Limit Theorem in practice*". \
In other words the question is asking: \
What is the probability that the difference of the true $p$-value to the estimate $\bar{X}$ i.e. the error $| \bar{X} - p|$, is smaller than 0.01, i.e. within an error range of 1%: $\mbox{Pr}(| \bar{X} - p| \ge .01) = 1- \mbox{Pr}(| \bar{X} - p| \le .01)$. \
A similar task was already computed in the mentioned chapter: $\mbox{Pr}(| \bar{X} - p| \le .01)$. Here, the answer to the question is 84% is the probability that the error is equal or larger than 0.01.

```{r}
N<-100
X <- 0.51
se <- sqrt(X*(1-X)/N)
error<-pnorm(0.01/se) - pnorm(-0.01/se)
1-error
```


## Exercises: Confidence Intervals p-values


>For these exercises, we will use actual polls from the 2016 election. You can load the data from the __dslabs__ package.
>
>```{r}
>library(dslabs)
>data("polls_us_election_2016")
>```
>
>Specifically, we will use all the national polls that ended within one week before the election.
>
>```{r, message=FALSE, message=FALSE}
>library(tidyverse)
>polls <- polls_us_election_2016 %>% 
>  filter(enddate >= "2016-10-31" & state == "U.S.") 
>```

### Qn
>1\. For the first poll, you can obtain the samples size and estimated Clinton percentage with:
>
>```{r, eval=FALSE}
>N <- polls$samplesize[1]
>x_hat <- polls$rawpoll_clinton[1]/100
>```
>
>Assume there are only two candidates and construct a 95% confidence interval for the election night proportion $p$. 

As stated in the book in chapter 15.6 "*Confidence intervals*", in order to compute the probability that p is within the 95% confidence interval one needs to compute the probability: 

$$
\mbox{Pr}\left(\bar{X} - 1.96\hat{\mbox{SE}}(\bar{X}) \leq p \leq \bar{X} + 1.96\hat{\mbox{SE}}(\bar{X})\right)
$$
The resulting confidence interval for p is has the margin error of 0.04 for the interval between 0.49 and 0.45.

```{r, message=FALSE}
library(tidyverse)
library(dslabs)
data("polls_us_election_2016")
polls <- polls_us_election_2016 %>% 
  filter(enddate >= "2016-10-31" & state == "U.S.") 
N <- polls$samplesize[1]
x_hat <- polls$rawpoll_clinton[1]/100
se_hat <- sqrt(x_hat * (1 - x_hat) / N)
c(x_hat - 1.96 * se_hat, x_hat + 1.96 * se_hat)
```

### Qn
>2\. Now use `dplyr` to add a confidence interval as two columns, call them `lower` and `upper`, to the object `poll`. Then use `select` to show the `pollster`, `enddate`, `x_hat`,`lower`, `upper` variables. Hint: define temporary columns `x_hat` and `se_hat`. 

The resulting table contains the confidence intervals for each pollster based on the average and standard error of their polls after the 31st of October 2016. The used equations are $\hat{X} = \hat{p}/100$, $\hat{\mbox{SE}}[\hat{X}] = \sqrt{\hat{X} (1-\hat{X})/N}$ and for the confidence interval $\hat{X} - 1.96\hat{\mbox{SE}}(\hat{X}) \leq p \leq \bar{X} + 1.96\hat{\mbox{SE}}(\hat{X})$.

```{r, message=FALSE}
library(tidyverse)
library(dslabs)
data("polls_us_election_2016")
polls <- polls_us_election_2016 %>% 
  filter(enddate >= "2016-10-31" & state == "U.S.") 
poll<-polls %>% select(pollster,enddate,samplesize,rawpoll_clinton) %>% mutate(x_hat=rawpoll_clinton/100)
poll<-poll %>% mutate(se_hat=sqrt(x_hat * (1 - x_hat) / samplesize))
poll<-poll %>% mutate(lower=x_hat - 1.96 * se_hat, upper=x_hat + 1.96 * se_hat)
poll %>% select(-rawpoll_clinton,-samplesize,-x_hat,-se_hat)
```

### Qn
>3\. The final tally for the popular vote was Clinton 48.2%	and Trump 46.1%. Add a column, call it `hit`, to the previous table stating if the confidence interval included the true proportion $p=0.482$ or not. 

Starting with the code of the previous exercise the task can be easily accomplished by using an `ifelse()` function.

```{r, message=FALSE}
library(tidyverse)
library(dslabs)
p<-0.482
data("polls_us_election_2016")
polls <- polls_us_election_2016 %>% 
  filter(enddate >= "2016-10-31" & state == "U.S.") 
poll<-polls %>% select(pollster,enddate,samplesize,rawpoll_clinton) %>% mutate(x_hat=rawpoll_clinton/100)
poll<-poll %>% mutate(se_hat=sqrt(x_hat * (1 - x_hat) / samplesize))
poll<-poll %>% mutate(lower=x_hat - 1.96 * se_hat, upper=x_hat + 1.96 * se_hat)
poll<-poll %>% select(-rawpoll_clinton,-samplesize)
poll %>% mutate(hit=ifelse(lower< p & upper > p,1,0)) %>% select(pollster,lower,upper,hit)
```

### Qn
>4\. For the table you just created, what proportion of confidence intervals included $p$?

To compute the proportion of confidence intervals that included p, one can simply use the `mean()` function for the `hit` column. The resulting value is 31% (0.3142857), that is the proportion of pollsters who managed to predict the final value with their polls. \
To illustrate this finding the following plot shows the poll number versus the predicted and final p-value (vertical black line). The predicted values are depicted with a small dot within the corresponding confidence interval. Only the green marked confidence intervals include the final p-value, which make up the 31% of the polls. The other 69% polls missed to predict the final p-value within the confidence interval.

```{r, message=FALSE}
library(tidyverse)
library(dslabs)
p<-0.482
data("polls_us_election_2016")
polls <- polls_us_election_2016 %>% 
  filter(enddate >= "2016-10-31" & state == "U.S.") 
poll<-polls %>% select(pollster,enddate,samplesize,rawpoll_clinton) %>% mutate(x_hat=rawpoll_clinton/100)
poll<-poll %>% mutate(se_hat=sqrt(x_hat * (1 - x_hat) / samplesize))
poll<-poll %>% mutate(lower=x_hat - 1.96 * se_hat, upper=x_hat + 1.96 * se_hat)
poll<-poll %>% select(-rawpoll_clinton,-samplesize)
poll<-poll %>% mutate(hit=ifelse(lower< p & upper > p,1,0)) %>%  mutate(pollnr=1:nrow(poll))
poll <- mutate(poll, p_inside = ifelse(hit, "Yes", "No") )
mean(poll$hit)
ggplot(poll, aes(pollnr, x_hat, ymin=lower, ymax=upper, col = p_inside)) + 
  geom_point(size=0.5)+
  geom_errorbar() + 
  coord_flip() + 
  geom_hline(yintercept = p)
```

### Qn
>5\. If these confidence intervals are constructed correctly, and the theory holds up, what proportion should include $p$?

If the computations and theory work as designed then the proportion of poll, which should include the predicted p-value is 95%. \
The basic assumption is that the random values are normally distributed. Then the confidence intervals are constructed in the way that it includes 95% of the values $X - 1.96\ \mbox{SE}(X) < p \leq X + 1.96\ \mbox{SE}(X)$. The value of 95% results from the factor 1.96, as described in chapter 15.6 "*Confidence intervals*":

```{r}
pnorm(1.96) - pnorm(-1.96)
```

### Qn
>6\. A much smaller proportion of the polls than expected produce confidence intervals containing $p$. If you look closely at the table, you will see that most polls that fail to include $p$ are underestimating. The reason for this is undecided voters, individuals polled that do not yet know who they will vote for or do not want to say. 
>Because, historically, undecideds divide evenly between the two main candidates on election day, it is more informative to estimate the spread or the difference between the proportion of two candidates $d$, which in this election was $0. 482 - 0.461 = 0.021$. 
>Assume that there are only two parties and that $d = 2p - 1$, redefine `polls` as below and
>re-do exercise 1, but for the difference.
>
>```{r, message=FALSE, comment=FALSE}
>polls <- polls_us_election_2016 %>% 
>  filter(enddate >= "2016-10-31" & state == "U.S.")  %>%
>  mutate(d_hat = rawpoll_clinton / 100 - rawpoll_trump / 100)
>```

Recall the standard error of the spread from exercise 15.1.8: $\mbox{SE}(d)= 2\sqrt{p(1-p)/N}$. Where $p$ is calculated by rearranging the definition of the spread to $p=(d-1)/2$. In exercise 1 it was asked to compute the confidence interval $\bar{X} - 1.96\hat{\mbox{SE}}(\bar{X}) < d \leq \bar{X} + 1.96\hat{\mbox{SE}}(\bar{X})$.\
Now, the confidence interval for the difference is between -0.16% and 8.2% with a margin of error of 8%.

```{r, message=FALSE}
library(tidyverse)
library(dslabs)
data("polls_us_election_2016")
polls <- polls_us_election_2016 %>% 
  filter(enddate >= "2016-10-31" & state == "U.S.")  %>%
  mutate(d_hat = rawpoll_clinton / 100 - rawpoll_trump / 100)
N <- polls$samplesize[1]
d_hat <- polls$d_hat[1]
x_hat<-(d_hat+1)/2
se_hat <- 2*sqrt(x_hat * (1 - x_hat) / N)
c(d_hat - 1.96 * se_hat, d_hat + 1.96 * se_hat)
```

### Qn
>7\. Now repeat exercise 3, but for the difference.

Back in exercise 3 it was asked to compute a parameter `hit`, which indicates weather the final p-value is within the estimated confidence interval of the poll. Now, in this exercise the computation is performed for the spread value d. 

```{r, message=FALSE}
library(tidyverse)
library(dslabs)
d<-0.021
data("polls_us_election_2016")
polls <- polls_us_election_2016 %>% 
  filter(enddate >= "2016-10-31" & state == "U.S.") %>%
  mutate(d_hat = rawpoll_clinton / 100 - rawpoll_trump / 100)
poll<-polls %>% select(pollster,enddate,samplesize,d_hat) 
poll<-poll %>% mutate(x_hat=(d_hat+1)/2)
poll<-poll %>% mutate(se_hat=2*sqrt(x_hat * (1 - x_hat) / samplesize))
poll<-poll %>% mutate(lower=d_hat - 1.96 * se_hat, upper=d_hat + 1.96 * se_hat)
poll<-poll %>% select(-samplesize)
poll<-poll %>% mutate(hit=ifelse(lower<= d & upper >= d,1,0)) %>% select(pollster,lower,upper,hit)
poll
```

### Qn
>8\. Now repeat exercise 4, but for the difference.

The equivalent question from exercise 4 would be "*What proportion of confidence intervals included $d$?*". \
The resulting value is 77% (0.7714286), which is the proportion of pollsters who managed to predict the final value with their polls. \
This result is also illustrated in the following plot. 

```{r, message=FALSE}
library(tidyverse)
library(dslabs)
d<-0.021
data("polls_us_election_2016")
polls <- polls_us_election_2016 %>% 
  filter(enddate >= "2016-10-31" & state == "U.S.") %>%
  mutate(d_hat = rawpoll_clinton / 100 - rawpoll_trump / 100)
poll<-polls %>% select(pollster,enddate,samplesize,d_hat) 
poll<-poll %>% mutate(x_hat=(d_hat+1)/2)
poll<-poll %>% mutate(se_hat=2*sqrt(x_hat * (1 - x_hat) / samplesize))
poll<-poll %>% mutate(lower=d_hat - 1.96 * se_hat, upper=d_hat + 1.96 * se_hat)
poll<-poll %>% select(-samplesize)
poll<-poll %>% mutate(hit=ifelse(lower<= d & upper >= d,1,0)) %>% select(pollster,lower,upper,hit,d_hat) %>%  arrange(pollster) %>% mutate(pollnr=1:nrow(poll))
poll <- mutate(poll, d_inside = ifelse(hit, "Yes", "No") )
mean(poll$hit)
ggplot(poll, aes(pollnr, d_hat, ymin=lower, ymax=upper, col = d_inside)) + 
  geom_point(size=0.5)+
  geom_errorbar() + 
  coord_flip() + 
  geom_hline(yintercept = d)
```

### Qn
>9\. Although the proportion of confidence intervals goes up substantially, it is still lower than 0.95. In the next chapter, we learn the reason for this. To motivate this, make a plot of the error, the difference between each poll's estimate and the actual $d=0.021$. Stratify by pollster.

In the plot the color (green and red) indicate, if the confidence interval of each poll was able to capture the final spread of $d=0.021$. \ 
The hint in this exercise that in the next chapter the reason for the large amount of pollsters missing to cover the final p- and d-value is called the *pollster bias*.

```{r, message=FALSE}
library(tidyverse)
library(dslabs)
d<-0.021
data("polls_us_election_2016")
polls <- polls_us_election_2016 %>% 
  filter(enddate >= "2016-10-31" & state == "U.S.") %>%
  mutate(d_hat = rawpoll_clinton / 100 - rawpoll_trump / 100)
poll<-polls %>% select(pollster,enddate,samplesize,d_hat) 
poll<-poll %>% mutate(x_hat=(d_hat+1)/2)
poll<-poll %>% mutate(se_hat=2*sqrt(x_hat * (1 - x_hat) / samplesize))
poll<-poll %>% mutate(lower=d_hat - 1.96 * se_hat, upper=d_hat + 1.96 * se_hat)
poll<-poll %>% select(-samplesize)
poll<-poll %>% mutate(hit=ifelse(lower<= d & upper >= d,1,0)) %>% select(pollster,lower,upper,hit,d_hat) %>%  arrange(pollster) %>% mutate(pollnr=1:nrow(poll))
poll<-mutate(poll, d_inside = ifelse(hit, "Yes", "No") )
poll<-mutate(poll,pollsternr=paste(pollnr,pollster,sep="_"))
poll$d_error<-poll$d_hat-0.02
ggplot(poll, aes(pollster, d_error, ymin=lower, ymax=upper, col = d_inside)) + 
  geom_point()+
  #geom_errorbar() + 
  coord_flip() + 
  geom_hline(yintercept = 0)
```

### Qn
>10\. Redo the plot that you made for exercise 9, but only for pollsters that took five or more polls.

Except for one pollster "USC Dornsife/LA Times" all other pollsters manage to get close to the true d-value of the spread. 

```{r, message=FALSE}
library(tidyverse)
library(dslabs)
d<-0.021
data("polls_us_election_2016")
polls <- polls_us_election_2016 %>% 
  filter(enddate >= "2016-10-31" & state == "U.S.") %>%
  mutate(d_hat = rawpoll_clinton / 100 - rawpoll_trump / 100) #%>% 
  #mutate(pollnr=1:nrow(poll))
pollsters <- polls %>% select(pollster) %>% group_by(pollster) %>% summarise(n=n()) %>% arrange(desc(n)) %>% filter(n>=5)
polls<-left_join(pollsters,polls) %>% select(pollster,n,samplesize,rawpoll_clinton,rawpoll_trump,d_hat)
poll<-polls %>% select(pollster,samplesize,d_hat) 
poll<-poll %>% mutate(x_hat=(d_hat+1)/2)
poll<-poll %>% mutate(se_hat=2*sqrt(x_hat * (1 - x_hat) / samplesize))
poll<-poll %>% mutate(lower=d_hat - 1.96 * se_hat, upper=d_hat + 1.96 * se_hat)
poll<-poll %>% select(-samplesize)
poll<-poll %>% mutate(hit=ifelse(lower<= d & upper >= d,1,0)) %>% select(pollster,d_hat,lower,upper,hit) %>%  mutate(pollnr=1:nrow(poll))
poll <- mutate(poll, d_inside = ifelse(hit, "Yes", "No") )
poll$d_error<-poll$d_hat-0.02
ggplot(poll, aes(pollster, d_error, ymin=lower, ymax=upper, col = d_inside)) + 
  geom_point()+
  #geom_errorbar() + 
  coord_flip() + 
  geom_hline(yintercept = 0)
```

## Exercises: Assosiation Tests

### Qn

>1\. A famous athlete has an impressive career, winning 70% of her 500 career matches. However, this athlete gets criticized because in important events, such as the Olympics, she has a losing record of 8 wins and 9 losses. Perform a Chi-square test to determine if this losing record can be simply due to chance as opposed to not performing well under pressure.

The Null-Hypothesis says then that the high loss rate is just due to chance. \
In order to perform the Chi-square ($\chi^2$) test, first a 2x2 table has to be constructed from the given information in the question. In regular tournaments there were 350 wins and 150 losses and in important tournaments there were 8 wins an 9 losses. \
With this data the function `chisq.test()` is fed. As a result three parameters are returned: the actual $\chi^2=3.0573$, degrees of freedom df=1 and the p-value =0.08038. \
The degrees of freedom are computed as a product of the dimensions of the table minus one. In this case the table has two rows and two columns, hence the dimension of a 2x2 matrix df=(n-1)(m-1), therefore the degrees of freedom are df=(2-1)(2-2)=1.

```{r}
reg_event_win<-500*0.7
reg_event_lose<-500*0.3
imp_event_win<-8
imp_event_lose<-9
two_by_two<-data.frame(reg=c(reg_event_win,reg_event_lose),imp=c(imp_event_win,imp_event_lose))
two_by_two
chisq_test <- two_by_two  %>% chisq.test()
chisq_test$p.value
chisq_test
````
```{r}
ind<-seq(0,8,0.5)
ind
plot(pchisq(ind, df =  1, lower.tail = FALSE))
```
Another example from the book, regarding the Chi-square test. 

```{r, message=FALSE, eval=FALSE}
library(tidyverse)
library(dslabs)
data("research_funding_rates")
research_funding_rates %>% select(discipline, applications_total, success_rates_total) 
totals <- research_funding_rates %>% 
  select(-discipline) %>% 
  summarize_all(sum) %>%
  summarize(yes_men = awards_men, 
            no_men = applications_men - awards_men, 
            yes_women = awards_women, 
            no_women = applications_women - awards_women) 
totals %>% summarize(percent_men = yes_men/(yes_men+no_men),
                     percent_women = yes_women/(yes_women+no_women))
rate <- totals %>%
  summarize(percent_total = 
              (yes_men + yes_women)/
              (yes_men + no_men +yes_women + no_women)) %>%
  pull(percent_total)
two_by_two <- data.frame(awarded = c("no", "yes"), 
                     men = c(totals$no_men, totals$yes_men),
                     women = c(totals$no_women, totals$yes_women))
two_by_two
data.frame(awarded = c("no", "yes"), 
       men = (totals$no_men + totals$yes_men) * c(1 - rate, rate),
       women = (totals$no_women + totals$yes_women) * c(1 - rate, rate))
chisq_test <- two_by_two %>% select(-awarded) %>% chisq.test()
chisq_test$p.value
chisq_test
```

### Qn
>2\. Why did we use the Chi-square test instead of Fisher's exact test in the previous exercise?
>
>a. It actually does not matter, since they give the exact same p-value.
>b. Fisher's exact and the Chi-square are different names for the same test.
>c. Because the sum of the rows and columns of the two-by-two table are not fixed so the hypergeometric distribution is not an appropriate assumption for the null hypothesis. For this reason, Fisher's exact test is rarely applicable with observational data.
>d. Because the Chi-square test runs faster.

Answer c, as stated in chapter *15.10.3 "Chi-square Test"*, the constrained that the values of rows and columns have to sum up to the same value. In the example of the "Lady Tasting Tea" it is always 4, where for the "Research Funding Rates" example this is not the case, as shown in the calculation below. Therefore, the hypergeometric distribution (random draws of $k$ successes in $n$ draws *without* replacement) cannot be applied to the "Research Funding Rates" example.

```{r, message=FALSE}
library(tidyverse)
library(dslabs)
data("research_funding_rates")
totals <- research_funding_rates %>% 
  select(-discipline) %>% 
  summarize_all(sum) %>%
  summarize(yes_men = awards_men, 
            no_men = applications_men - awards_men, 
            yes_women = awards_women, 
            no_women = applications_women - awards_women) 
two_by_two <- data.frame(awarded = c("no", "yes"), 
                     men = c(totals$no_men, totals$yes_men),
                     women = c(totals$no_women, totals$yes_women))
two_by_two<- two_by_two %>% select(-awarded)
two_by_two<-rbind(two_by_two, total=colSums(two_by_two)) 
two_by_two<-cbind(two_by_two, total=rowSums(two_by_two))  
data.frame(awarded =c("yes","no","total"),two_by_two)
```

### Qn
>3\. Compute the odds ratio of "losing under pressure" along with a confidence interval.

The result shows, that it is very unlikely that the athlete loses just by chance. The effect "losing under pressure" is present that the probability for the athlete to lose an important match is higher than a regular match \
The odds for the regular matches is 2.3 and that of an important match 0.8. Therefore, the odds ratio of these values is 2.6. The confidence interval, according to the protocol in chapter 15.10.5 *Confidence intervals for the odds ratio*, is between 0.994 and 6.93 (MOE=5.94) with a p-value of 0.051. Here, 1 is included in the confidence interval, therefore the p-value is larger than 0.05. 

```{r}
reg_event_win<-500*0.7
reg_event_lose<-500*0.3
imp_event_win<-8
imp_event_lose<-9
two_by_two<-data.frame(result=c("win","lose"),reg=c(reg_event_win,reg_event_lose),imp=c(imp_event_win,imp_event_lose))
two_by_two

odds_reg <- with(two_by_two, (reg[1]/sum(reg)) / (reg[2]/sum(reg)))
odds_reg
odds_imp <- with(two_by_two, (imp[1]/sum(imp)) / (imp[2]/sum(imp)))
odds_imp

# odds ratio
odds_reg/odds_imp

# compute the confidence interval
log_or <- log(odds_reg / odds_imp)
se <- two_by_two %>% select(-result) %>%
  summarize(se = sqrt(sum(1/reg) + sum(1/imp))) %>%
  pull(se)
ci <- log_or + c(-1,1) * qnorm(0.975) * se
exp(ci)
# compute the p-value
2*(1 - pnorm(log_or, 0, se))
````

  
### Qn
>4\. Notice that the p-value is larger than 0.05 but the 95% confidence interval does not include 1. What explains this?
>
>a. We made a mistake in our code.
>b. These are based on t-statistics so the connection between p-value and confidence intervals does not apply.
>c. Different approximations are used for the p-value and the confidence interval calculation. If we had a larger sample size the match would be better.
>d. We should use the Fisher exact test to get confidence intervals.

Answer c, because here the estimated p-value is 0.052 with a confidence interval of 0.9937118 to 6.9342286. \

As stated in the question, the confidence interval does not include the "null value" 1, therefore the value, which is formulated by the "null hypothesis" of the odds ratio. The term "null value" means also that no effect can be observed. An odds ratio of 1 means that neither of the two odds is dominant. In other words, there is no causal relationship between the observed variables and differences can be accounted due to chances. If the confident interval does not contain the null value, here it is 1, then the test is statistically significant, such that the observed effect can not be accounted to chance. But if the confidence interval does contain the null value then the observed result can be attributed to chance, because it is possible that no effect is observed. \

The p-value ranges between 0 and 1 and can be expressed in percentage values. If the p-value is below 5% it is said that it is statistically significant such that the null hypothesis will be rejected. \

In addition, in chapter 15.9 *p-values* it is stated:
"*Keep in mind that there is a close connection between p-values and confidence intervals. If a 95% confidence interval of the spread does not include 0, we know that the p-value must be smaller than 0.05.*". \
Therefore, here the no-effect value/null value is the number 0, if it is not included in the confidence interval then an effect can indeed be observed. For the p-value it means that if the p-value is below 5% then the chances of no-effect is very low. Since both measures the confidence interval and the p-value are closely related, there is a correlation between these two measures. Meaning, if the p-value is below 5% then the confidence interval cannot contain the null value and vice versa. If the confidence interval does not contain the null value, then the p-value will be below 5%. 

### Qn
>5\. Multiply the two-by-two table by 2 and see if the p-value and confidence retrieval are a better match.

The purpose by multiplying the table by 2 the sample size is to increase the sample size and improve the result according to the CLT prediction. \

The confidence interval has changed significantly to the range of 1.32 to 5.2 with a narrower mean of error with a value of 3.9. Now, the value 1 is not included in the confidence interval which is why the p-value below 0.05. The computed p-value is 0.0059, which confirms the "lose under pressure" hypothesis. \
The hypothesis states, that at important events the athlete tends to lose. \

15.10.7 "Large samples, small p-values": *Note that the relationship between odds ratio and p-value is not one-to-one. It depends on the sample size. So a very small p-value does not necessarily mean a very large odds ratio.*. This statement in the book is proved by this exercise.

```{r}
reg_event_win<-500*0.7
reg_event_lose<-500*0.3
imp_event_win<-8
imp_event_lose<-9
two_by_two<-data.frame(result=c("win","lose"),reg=2*c(reg_event_win,reg_event_lose),imp=2*c(imp_event_win,imp_event_lose))
two_by_two
odds_reg <- with(two_by_two, (reg[1]/sum(reg)) / (reg[2]/sum(reg)))
odds_reg
odds_imp <- with(two_by_two, (imp[1]/sum(imp)) / (imp[2]/sum(imp)))
odds_imp
odds_reg/odds_imp

log_or <- log(odds_reg / odds_imp)
se <- two_by_two %>% select(-result) %>%
  summarize(se = sqrt(sum(1/reg) + sum(1/imp))) %>%
  pull(se)
ci <- log_or + c(-1,1) * qnorm(0.975) * se
exp(ci)
2*(1 - pnorm(log_or, 0, se))
````

